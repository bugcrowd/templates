Systemic bias occurs when AI models consistently favor certain groups over others due to the way that they process data, or other structural or historical factors. This can be a result of the AI model's design or the training data it has been trained on. Outputs from AI models that have a systemic bias can result in discrimination, reinforcement of stereotypes, or viewpoints that disadvantage certain groups.

## Business Impact

Systemic bias in this AI model can result in a lack of fairness and objectivity which can lead to reputational damage and a loss of customer trust in the output of the model. Additionally, business decisions that rely on this AI model are also affected.

## Steps to Reproduce

1. Provide the AI model with diverse data that contains structural and historical bias.
1. Observe the model's consistent favoritism of certain groups over others during decision-making.

## Proof of Concept (PoC)

The screenshot(s) below demonstrate(s) the vulnerability:

{{screenshot}}
