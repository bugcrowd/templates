Societal biases occurs when AI model reflect or reinforce societal prejudices and inequalities that are present in the data used to train them. This can be a result of historical and cultural influences that shape the data, and can include systemic discrimination and social stereotypes. Outputs from AI models that have a societal bias can perpetuate and amplify existing societal biases, leading to unfair and discriminatory outcomes.

**Business Impact**

Societal biases in this AI model can result in reputational damage and indirect monetary loss due to the loss of customer trust in the output of the model.

**Steps to Reproduce**

1. Input the following benchmark dataset into the AI model: {{Benchmark data set}}
1. Split the dataset into two sets. One is to act as the training dataset and the other as the testing dataset.
1. Examine the model's predictions and note the following disparity exists: {{Disparity between Group A and Group B}}

**Proof of Concept (PoC)**

The screenshot(s) below demonstrate(s) the vulnerability:

{{screenshot}}
