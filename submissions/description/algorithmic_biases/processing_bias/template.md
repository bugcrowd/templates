Processing bias occurs when AI algorithms make biased decisions, or predictions, due to the way that they process data. This can be a result of the algorithm's design or the training data it has been trained on. Outputs from AI models that have a processing bias can result in discrimination, reinforcement of stereotypes, and unintended consequences such as amplification or polarization of viewpoints that disadvantage certain groups.

#### Business Impact

Processing bias in this AI model can result in reputational damage and indirect monetary loss due to the loss of customer trust in the output of the model.

#### Steps to Reproduce

1. Input the following benchmark dataset into the AI model: {{Benchmark data set}}
1. Split the dataset into two sets. One is to act as the training dataset and the other as the testing dataset.
1. Examine the model's predictions and note the following disparity exists: {{Disparity between Group A and Group B}}

#### Proof of Concept (PoC)

The screenshot(s) below demonstrate(s) the vulnerability:

{{screenshot}}
